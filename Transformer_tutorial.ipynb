{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8296e231-f46d-4646-8821-6a188a7bb1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family=\"NanumBarunGothic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab065354-2ca1-4fe1-9eb3-c039534616b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from transformers import MarianMTModel, MarianTokenizer # translation machine\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math, random\n",
    "from einops import rearrange\n",
    "\n",
    "DEVICE = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c8bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # Multi-GPU case\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da12c12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoya9\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hoya9\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-ko-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer & model\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n",
    "model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ko-en') # MT: Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158f7887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_idx =  0\n",
      "pad_idx =  65000\n"
     ]
    }
   ],
   "source": [
    "eos_idx = tokenizer.eos_token_id\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "print('eos_idx = ', eos_idx)\n",
    "print('pad_idx = ', pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b90c7",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9f2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1bjbeWgqlVKJ9gzDcL1hfD9cIItQy9_N2\n",
      "From (redirected): https://drive.google.com/uc?id=1bjbeWgqlVKJ9gzDcL1hfD9cIItQy9_N2&confirm=t&uuid=c6900891-3536-4941-8858-fccfc20e6e47\n",
      "To: c:\\Users\\hoya9\\OneDrive\\Desktop\\torch-gpu\\Workspace\\Transformer_small.pt\n",
      "\n",
      "  0%|          | 0.00/448M [00:00<?, ?B/s]\n",
      "  0%|          | 524k/448M [00:00<03:27, 2.16MB/s]\n",
      "  0%|          | 1.05M/448M [00:00<02:44, 2.72MB/s]\n",
      "  0%|          | 1.57M/448M [00:00<02:16, 3.28MB/s]\n",
      "  1%|          | 4.19M/448M [00:00<00:45, 9.78MB/s]\n",
      "  2%|▏         | 8.39M/448M [00:00<00:26, 16.7MB/s]\n",
      "  4%|▎         | 16.3M/448M [00:00<00:13, 33.1MB/s]\n",
      "  5%|▍         | 20.4M/448M [00:01<00:23, 18.4MB/s]\n",
      "  6%|▌         | 25.7M/448M [00:01<00:22, 18.8MB/s]\n",
      "  7%|▋         | 31.5M/448M [00:01<00:16, 24.8MB/s]\n",
      "  8%|▊         | 35.1M/448M [00:01<00:20, 20.3MB/s]\n",
      "  9%|▊         | 38.3M/448M [00:02<00:22, 18.4MB/s]\n",
      " 10%|▉         | 44.0M/448M [00:02<00:16, 24.8MB/s]\n",
      " 11%|█         | 49.8M/448M [00:02<00:12, 30.9MB/s]\n",
      " 12%|█▏        | 54.0M/448M [00:02<00:15, 26.1MB/s]\n",
      " 13%|█▎        | 59.2M/448M [00:02<00:12, 31.0MB/s]\n",
      " 15%|█▍        | 65.5M/448M [00:02<00:10, 37.0MB/s]\n",
      " 16%|█▌        | 71.3M/448M [00:02<00:09, 41.3MB/s]\n",
      " 17%|█▋        | 76.5M/448M [00:03<00:10, 36.5MB/s]\n",
      " 18%|█▊        | 80.7M/448M [00:03<00:15, 24.3MB/s]\n",
      " 19%|█▉        | 86.5M/448M [00:03<00:12, 30.0MB/s]\n",
      " 20%|██        | 90.7M/448M [00:03<00:13, 26.0MB/s]\n",
      " 21%|██        | 94.4M/448M [00:03<00:13, 25.6MB/s]\n",
      " 22%|██▏       | 99.1M/448M [00:04<00:13, 26.0MB/s]\n",
      " 23%|██▎       | 105M/448M [00:04<00:10, 31.8MB/s] \n",
      " 24%|██▍       | 109M/448M [00:04<00:14, 23.7MB/s]\n",
      " 26%|██▌       | 115M/448M [00:04<00:11, 29.4MB/s]\n",
      " 27%|██▋       | 119M/448M [00:04<00:14, 22.0MB/s]\n",
      " 28%|██▊       | 125M/448M [00:05<00:11, 27.2MB/s]\n",
      " 29%|██▊       | 128M/448M [00:05<00:11, 27.0MB/s]\n",
      " 29%|██▉       | 132M/448M [00:05<00:11, 28.0MB/s]\n",
      " 30%|███       | 136M/448M [00:05<00:13, 22.5MB/s]\n",
      " 31%|███▏      | 141M/448M [00:05<00:13, 22.9MB/s]\n",
      " 32%|███▏      | 145M/448M [00:05<00:11, 26.2MB/s]\n",
      " 33%|███▎      | 149M/448M [00:06<00:10, 28.5MB/s]\n",
      " 34%|███▍      | 155M/448M [00:06<00:08, 33.0MB/s]\n",
      " 36%|███▌      | 160M/448M [00:06<00:08, 33.9MB/s]\n",
      " 37%|███▋      | 166M/448M [00:06<00:07, 39.3MB/s]\n",
      " 38%|███▊      | 170M/448M [00:06<00:08, 32.9MB/s]\n",
      " 39%|███▉      | 175M/448M [00:06<00:08, 32.1MB/s]\n",
      " 40%|███▉      | 178M/448M [00:06<00:08, 31.1MB/s]\n",
      " 41%|████      | 182M/448M [00:06<00:08, 31.3MB/s]\n",
      " 42%|████▏     | 188M/448M [00:07<00:06, 37.5MB/s]\n",
      " 43%|████▎     | 193M/448M [00:07<00:06, 41.3MB/s]\n",
      " 44%|████▍     | 199M/448M [00:07<00:05, 45.1MB/s]\n",
      " 45%|████▌     | 204M/448M [00:07<00:05, 46.1MB/s]\n",
      " 47%|████▋     | 209M/448M [00:07<00:05, 40.8MB/s]\n",
      " 48%|████▊     | 215M/448M [00:07<00:05, 44.3MB/s]\n",
      " 49%|████▉     | 220M/448M [00:07<00:05, 45.0MB/s]\n",
      " 50%|█████     | 224M/448M [00:07<00:05, 41.4MB/s]\n",
      " 51%|█████▏    | 230M/448M [00:07<00:04, 44.9MB/s]\n",
      " 53%|█████▎    | 236M/448M [00:08<00:04, 47.6MB/s]\n",
      " 54%|█████▍    | 241M/448M [00:08<00:04, 47.6MB/s]\n",
      " 55%|█████▍    | 246M/448M [00:08<00:04, 48.9MB/s]\n",
      " 56%|█████▌    | 252M/448M [00:08<00:06, 29.2MB/s]\n",
      " 57%|█████▋    | 256M/448M [00:08<00:06, 29.1MB/s]\n",
      " 58%|█████▊    | 260M/448M [00:08<00:06, 29.8MB/s]\n",
      " 59%|█████▉    | 266M/448M [00:09<00:05, 35.0MB/s]\n",
      " 60%|██████    | 270M/448M [00:09<00:05, 35.4MB/s]\n",
      " 61%|██████    | 274M/448M [00:09<00:06, 26.9MB/s]\n",
      " 62%|██████▏   | 278M/448M [00:09<00:08, 19.0MB/s]\n",
      " 63%|██████▎   | 283M/448M [00:09<00:07, 23.4MB/s]\n",
      " 64%|██████▍   | 288M/448M [00:09<00:05, 29.4MB/s]\n",
      " 65%|██████▌   | 294M/448M [00:10<00:04, 32.2MB/s]\n",
      " 67%|██████▋   | 299M/448M [00:10<00:03, 37.6MB/s]\n",
      " 68%|██████▊   | 304M/448M [00:10<00:03, 36.7MB/s]\n",
      " 69%|██████▉   | 308M/448M [00:10<00:05, 27.6MB/s]\n",
      " 70%|██████▉   | 314M/448M [00:10<00:04, 31.4MB/s]\n",
      " 71%|███████   | 318M/448M [00:10<00:04, 30.3MB/s]\n",
      " 72%|███████▏  | 322M/448M [00:10<00:03, 32.2MB/s]\n",
      " 73%|███████▎  | 326M/448M [00:11<00:03, 31.0MB/s]\n",
      " 73%|███████▎  | 329M/448M [00:11<00:03, 31.0MB/s]\n",
      " 74%|███████▍  | 334M/448M [00:11<00:03, 29.9MB/s]\n",
      " 75%|███████▌  | 338M/448M [00:11<00:03, 31.4MB/s]\n",
      " 76%|███████▋  | 342M/448M [00:11<00:03, 28.4MB/s]\n",
      " 77%|███████▋  | 346M/448M [00:11<00:03, 27.9MB/s]\n",
      " 78%|███████▊  | 350M/448M [00:11<00:03, 31.8MB/s]\n",
      " 79%|███████▉  | 354M/448M [00:12<00:03, 25.3MB/s]\n",
      " 80%|███████▉  | 359M/448M [00:12<00:03, 29.7MB/s]\n",
      " 81%|████████  | 362M/448M [00:12<00:02, 30.9MB/s]\n",
      " 82%|████████▏ | 368M/448M [00:12<00:02, 36.8MB/s]\n",
      " 83%|████████▎ | 373M/448M [00:12<00:01, 38.6MB/s]\n",
      " 84%|████████▍ | 377M/448M [00:12<00:02, 27.0MB/s]\n",
      " 85%|████████▌ | 382M/448M [00:12<00:02, 32.1MB/s]\n",
      " 86%|████████▌ | 386M/448M [00:13<00:02, 27.7MB/s]\n",
      " 87%|████████▋ | 391M/448M [00:13<00:02, 24.1MB/s]\n",
      " 88%|████████▊ | 395M/448M [00:13<00:01, 27.2MB/s]\n",
      " 90%|████████▉ | 403M/448M [00:13<00:01, 37.1MB/s]\n",
      " 91%|█████████ | 409M/448M [00:13<00:00, 41.2MB/s]\n",
      " 92%|█████████▏| 414M/448M [00:13<00:00, 37.1MB/s]\n",
      " 94%|█████████▎| 419M/448M [00:14<00:00, 41.2MB/s]\n",
      " 95%|█████████▍| 424M/448M [00:14<00:00, 40.0MB/s]\n",
      " 96%|█████████▌| 429M/448M [00:14<00:00, 42.2MB/s]\n",
      " 97%|█████████▋| 434M/448M [00:14<00:00, 26.4MB/s]\n",
      " 98%|█████████▊| 438M/448M [00:14<00:00, 19.5MB/s]\n",
      " 98%|█████████▊| 441M/448M [00:15<00:00, 12.0MB/s]\n",
      "100%|█████████▉| 446M/448M [00:15<00:00, 16.3MB/s]\n",
      "100%|██████████| 448M/448M [00:15<00:00, 28.5MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1M0yYP2umxlwaAbk_iq5G_Z5y3qLu9Wet\n",
      "To: c:\\Users\\hoya9\\OneDrive\\Desktop\\torch-gpu\\Workspace\\Transformer_small_history.pt\n",
      "\n",
      "  0%|          | 0.00/1.39k [00:00<?, ?B/s]\n",
      "100%|██████████| 1.39k/1.39k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64 # 실제 논문에선 2.5만 token이 한 batch에 담기게 했다고 함\n",
    "LAMBDA = 0 # l2-Regularization를 위한 hyperparams (저장된 모델)\n",
    "EPOCH = 15 # 저장된 모델\n",
    "max_len = 100 # 길이 제한 (GPU 부담도 많이 감소)\n",
    "\"\"\"\n",
    "decoder에서 모델이 <eos>가 출력되는 것까지는 Loss를 구하고,\n",
    "그뒤 <pad>에 대한 Loss는 무시하기 위한 역활\n",
    "즉, label이 <pad> 일 때는 무시\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "scheduler_name = 'Noam' # Transformer 논문 제 2저자가 제안한 learning rate scheduler\n",
    "scheduler_name = 'Cos'\n",
    "#### Noam ####\n",
    "# warmup_steps = 4000 # 실제 논문에서 제시한 값 (총 10만 step의 4%)\n",
    "warmup_steps = 1000 # 데이터 수 * EPOCH / BS = 총 step 수 인것 고려 (저장된 모델)\n",
    "LR_scale = .5 # Noam scheduler에 peak LR 값 조절을 위해 곱해질 값 (저장된 모델)\n",
    "#### Cos ####\n",
    "LR_init = 5e-4\n",
    "T0 = 1500 # 첫 주기\n",
    "T_mult = 2 # 배 만큼 주기가 길어짐 (1보다 큰 정수여야 함)\n",
    "#############\n",
    "\n",
    "\"\"\"\n",
    "나만의 모델을 만들고 싶으면,\n",
    "new_model_train = True\n",
    "prev_model_use = False\n",
    "\"\"\"\n",
    "new_model_train = False\n",
    "prev_model_use = True\n",
    "\n",
    "if prev_model_use:\n",
    "    !gdown https://drive.google.com/uc?id=1bjbeWgqlVKJ9gzDcL1hfD9cIItQy9_N2 -O Transformer_small.pt\n",
    "    !gdown https://drive.google.com/uc?id=1M0yYP2umxlwaAbk_iq5G_Z5y3qLu9Wet -O Transformer_small_history.pt\n",
    "\n",
    "    save_model_path = 'Transformer_small.pt'\n",
    "    save_history_path = 'Transformer_small_history.pt'\n",
    "else:\n",
    "    save_model=path = 'Transformer_small2.pt'\n",
    "    save_history_path = 'Transformer_small2_history.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1803d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "논문에 나오는 BASE MODEL\n",
    "train loss를 많이 줄이려면 많은 epoch이 요구됨.\n",
    "test 성능도 높이려면 더 많은 데이터가 필요함\n",
    "\"\"\"\n",
    "n_layers = 6\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n_heads = 8\n",
    "drop_p = .1\n",
    "\n",
    "# 좀 사이즈 줄인 모델 (훈련된 input_embedding, fc_out 사용하면 사용 불가)\n",
    "n_layers = 3\n",
    "d_model = 26\n",
    "d_ff = 512\n",
    "n_heads = 8\n",
    "drop_p = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bcc2e5",
   "metadata": {},
   "source": [
    "### 토크나이저 & 학습된 모델 써보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0d2ecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁H', 'i', ',', '▁I', \"'\", 'm', '▁I', 'an', '.', '▁', '...', '▁a', '▁a', '?']\n",
      "['▁a', '/', 'b', '▁1', '+2', '+3', '▁2:', '1', '▁a', '>', 'b']\n",
      "['▁p', 're', 'tra', 'in', 'ed', '▁re', 'st', 'art']\n",
      "['▁ch', 'at', 'G', 'P', 'T']\n",
      "['▁The', '▁', 'ex', 'am', 'ple', '▁is', '▁', 'very', '▁good', '▁in', '▁', 'our', '▁', 'le', 'c', 'ture']\n",
      "['▁한', '글', '은', '▁어떻게', '▁할까', '?']\n",
      "['▁확실히', '▁띄', '어', '쓰기', '▁기준으로', '▁토', '크', '나이', '징', '을', '▁하는', '▁것', '▁같', '진', '▁않다', '.']\n",
      "['▁여러분', '▁차례', '!']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 써보기 (_로 띄어쓰기를 나타낸다! 즉, _가 없으면 이어진 한 단어 : subword tokenizing)\n",
    "# tokenizer에 대한 참고 자료: https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/\n",
    "print(tokenizer.tokenize(\"Hi, I'm Ian. ... a  a?\"))\n",
    "print(tokenizer.tokenize('a/b 1+2+3 2:1 a>b'))\n",
    "print(tokenizer.tokenize('pretrained restart'))\n",
    "print(tokenizer.tokenize('chatGPT'))\n",
    "print(tokenizer.tokenize('The example is very good in our lecture'))\n",
    "print(tokenizer.tokenize('한글은 어떻게 할까?'))\n",
    "print(tokenizer.tokenize('확실히 띄어쓰기 기준으로 토크나이징을 하는 것 같진 않다.'))\n",
    "print(tokenizer.tokenize('여러분 차례!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c14f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
