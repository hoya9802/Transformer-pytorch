{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8296e231-f46d-4646-8821-6a188a7bb1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family=\"NanumBarunGothic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab065354-2ca1-4fe1-9eb3-c039534616b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoya9\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from transformers import MarianMTModel, MarianTokenizer # translation machine\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math, random\n",
    "from einops import rearrange\n",
    "\n",
    "DEVICE = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c8bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # Multi-GPU case\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da12c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer & model\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n",
    "model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ko-en') # MT: Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "158f7887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_idx =  0\n",
      "pad_idx =  65000\n"
     ]
    }
   ],
   "source": [
    "eos_idx = tokenizer.eos_token_id\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "print('eos_idx = ', eos_idx)\n",
    "print('pad_idx = ', pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b90c7",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc9f2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file data already exists.\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1bjbeWgqlVKJ9gzDcL1hfD9cIItQy9_N2\n",
      "From (redirected): https://drive.google.com/uc?id=1bjbeWgqlVKJ9gzDcL1hfD9cIItQy9_N2&confirm=t&uuid=32475125-3a81-4844-8f38-6bd97fb6adb5\n",
      "To: c:\\Users\\hoya9\\OneDrive\\Desktop\\torch-gpu\\Workspace\\Transformer-pytorch\\data\\Transformer_small.pt\n",
      "\n",
      "  0%|          | 0.00/448M [00:00<?, ?B/s]\n",
      "  0%|          | 524k/448M [00:00<03:31, 2.12MB/s]\n",
      "  0%|          | 1.57M/448M [00:00<01:32, 4.83MB/s]\n",
      "  1%|          | 2.62M/448M [00:00<01:16, 5.83MB/s]\n",
      "  1%|          | 4.19M/448M [00:00<00:55, 8.03MB/s]\n",
      "  1%|▏         | 6.29M/448M [00:00<00:41, 10.7MB/s]\n",
      "  2%|▏         | 7.86M/448M [00:00<00:38, 11.5MB/s]\n",
      "  2%|▏         | 9.96M/448M [00:01<00:35, 12.5MB/s]\n",
      "  3%|▎         | 11.5M/448M [00:01<00:34, 12.7MB/s]\n",
      "  3%|▎         | 13.6M/448M [00:01<00:37, 11.6MB/s]\n",
      "  3%|▎         | 15.2M/448M [00:01<00:35, 12.3MB/s]\n",
      "  4%|▍         | 17.3M/448M [00:01<00:29, 14.4MB/s]\n",
      "  4%|▍         | 18.9M/448M [00:01<00:29, 14.5MB/s]\n",
      "  5%|▍         | 20.4M/448M [00:01<00:32, 13.1MB/s]\n",
      "  5%|▍         | 22.0M/448M [00:01<00:36, 11.8MB/s]\n",
      "  5%|▌         | 23.6M/448M [00:02<00:36, 11.8MB/s]\n",
      "  6%|▌         | 25.2M/448M [00:02<00:39, 10.6MB/s]\n",
      "  6%|▌         | 26.7M/448M [00:02<00:39, 10.8MB/s]\n",
      "  6%|▋         | 28.3M/448M [00:02<00:47, 8.85MB/s]\n",
      "  7%|▋         | 29.9M/448M [00:02<00:41, 10.1MB/s]\n",
      "  7%|▋         | 31.5M/448M [00:02<00:40, 10.3MB/s]\n",
      "  7%|▋         | 33.0M/448M [00:03<00:43, 9.64MB/s]\n",
      "  8%|▊         | 34.1M/448M [00:03<00:42, 9.77MB/s]\n",
      "  8%|▊         | 35.1M/448M [00:03<00:44, 9.29MB/s]\n",
      "  8%|▊         | 36.2M/448M [00:03<00:45, 9.06MB/s]\n",
      "  8%|▊         | 37.2M/448M [00:03<00:46, 8.90MB/s]\n",
      "  9%|▊         | 38.3M/448M [00:03<00:48, 8.52MB/s]\n",
      "  9%|▉         | 39.3M/448M [00:03<00:54, 7.50MB/s]\n",
      "  9%|▉         | 40.4M/448M [00:04<00:52, 7.71MB/s]\n",
      "  9%|▉         | 41.4M/448M [00:04<00:49, 8.27MB/s]\n",
      "  9%|▉         | 42.5M/448M [00:04<00:52, 7.66MB/s]\n",
      " 10%|▉         | 43.5M/448M [00:04<00:57, 7.01MB/s]\n",
      " 10%|▉         | 44.6M/448M [00:04<01:01, 6.52MB/s]\n",
      " 10%|█         | 45.6M/448M [00:04<01:02, 6.47MB/s]\n",
      " 10%|█         | 46.7M/448M [00:05<01:11, 5.64MB/s]\n",
      " 11%|█         | 47.7M/448M [00:05<01:20, 5.00MB/s]\n",
      " 11%|█         | 48.8M/448M [00:05<01:22, 4.85MB/s]\n",
      " 11%|█         | 49.3M/448M [00:05<01:31, 4.37MB/s]\n",
      " 11%|█         | 50.3M/448M [00:06<01:29, 4.46MB/s]\n",
      " 11%|█▏        | 51.4M/448M [00:06<01:12, 5.47MB/s]\n",
      " 12%|█▏        | 53.5M/448M [00:06<00:49, 8.04MB/s]\n",
      " 12%|█▏        | 55.6M/448M [00:06<00:38, 10.1MB/s]\n",
      " 13%|█▎        | 57.7M/448M [00:06<00:33, 11.8MB/s]\n",
      " 13%|█▎        | 59.8M/448M [00:06<00:29, 13.1MB/s]\n",
      " 14%|█▍        | 61.9M/448M [00:06<00:27, 14.2MB/s]\n",
      " 14%|█▍        | 64.0M/448M [00:06<00:25, 15.0MB/s]\n",
      " 15%|█▍        | 66.1M/448M [00:06<00:24, 15.7MB/s]\n",
      " 15%|█▌        | 68.2M/448M [00:07<00:23, 16.1MB/s]\n",
      " 16%|█▌        | 70.3M/448M [00:07<00:22, 16.5MB/s]\n",
      " 16%|█▌        | 72.4M/448M [00:07<00:22, 16.9MB/s]\n",
      " 17%|█▋        | 74.4M/448M [00:07<00:21, 17.1MB/s]\n",
      " 17%|█▋        | 76.5M/448M [00:07<00:21, 17.4MB/s]\n",
      " 18%|█▊        | 78.6M/448M [00:07<00:21, 17.5MB/s]\n",
      " 18%|█▊        | 80.7M/448M [00:07<00:20, 17.6MB/s]\n",
      " 18%|█▊        | 82.8M/448M [00:07<00:20, 17.8MB/s]\n",
      " 19%|█▉        | 84.9M/448M [00:08<00:20, 17.8MB/s]\n",
      " 19%|█▉        | 87.0M/448M [00:08<00:20, 17.9MB/s]\n",
      " 20%|█▉        | 89.1M/448M [00:08<00:20, 18.0MB/s]\n",
      " 20%|██        | 91.2M/448M [00:08<00:19, 17.9MB/s]\n",
      " 21%|██        | 93.3M/448M [00:08<00:19, 18.2MB/s]\n",
      " 21%|██▏       | 95.9M/448M [00:08<00:18, 19.4MB/s]\n",
      " 22%|██▏       | 98.0M/448M [00:08<00:19, 18.4MB/s]\n",
      " 23%|██▎       | 101M/448M [00:08<00:16, 20.8MB/s] \n",
      " 23%|██▎       | 104M/448M [00:08<00:15, 22.5MB/s]\n",
      " 24%|██▍       | 107M/448M [00:09<00:14, 23.5MB/s]\n",
      " 25%|██▍       | 110M/448M [00:09<00:13, 24.7MB/s]\n",
      " 25%|██▌       | 113M/448M [00:09<00:13, 25.0MB/s]\n",
      " 26%|██▌       | 115M/448M [00:09<00:13, 25.0MB/s]\n",
      " 26%|██▋       | 118M/448M [00:09<00:13, 25.2MB/s]\n",
      " 27%|██▋       | 121M/448M [00:09<00:12, 25.7MB/s]\n",
      " 28%|██▊       | 124M/448M [00:09<00:12, 25.7MB/s]\n",
      " 28%|██▊       | 126M/448M [00:10<00:18, 17.3MB/s]\n",
      " 29%|██▉       | 129M/448M [00:10<00:16, 19.6MB/s]\n",
      " 30%|██▉       | 133M/448M [00:10<00:14, 21.3MB/s]\n",
      " 30%|███       | 136M/448M [00:10<00:13, 22.8MB/s]\n",
      " 31%|███       | 139M/448M [00:10<00:12, 23.9MB/s]\n",
      " 32%|███▏      | 142M/448M [00:10<00:12, 24.7MB/s]\n",
      " 32%|███▏      | 145M/448M [00:10<00:11, 25.3MB/s]\n",
      " 33%|███▎      | 148M/448M [00:10<00:11, 25.6MB/s]\n",
      " 34%|███▎      | 151M/448M [00:10<00:11, 25.3MB/s]\n",
      " 34%|███▍      | 154M/448M [00:11<00:11, 25.5MB/s]\n",
      " 35%|███▌      | 157M/448M [00:11<00:11, 25.8MB/s]\n",
      " 36%|███▌      | 160M/448M [00:11<00:11, 26.1MB/s]\n",
      " 36%|███▋      | 164M/448M [00:11<00:10, 27.4MB/s]\n",
      " 37%|███▋      | 167M/448M [00:11<00:09, 28.3MB/s]\n",
      " 38%|███▊      | 170M/448M [00:11<00:09, 29.6MB/s]\n",
      " 39%|███▉      | 175M/448M [00:11<00:08, 32.5MB/s]\n",
      " 40%|████      | 180M/448M [00:11<00:06, 38.9MB/s]\n",
      " 42%|████▏     | 187M/448M [00:12<00:09, 27.3MB/s]\n",
      " 43%|████▎     | 193M/448M [00:12<00:07, 33.8MB/s]\n",
      " 44%|████▍     | 199M/448M [00:12<00:06, 39.4MB/s]\n",
      " 46%|████▌     | 206M/448M [00:12<00:05, 44.3MB/s]\n",
      " 47%|████▋     | 211M/448M [00:12<00:05, 47.4MB/s]\n",
      " 49%|████▊     | 218M/448M [00:12<00:04, 50.7MB/s]\n",
      " 50%|████▉     | 223M/448M [00:12<00:04, 51.0MB/s]\n",
      " 51%|█████     | 229M/448M [00:12<00:04, 52.5MB/s]\n",
      " 52%|█████▏    | 235M/448M [00:13<00:03, 53.8MB/s]\n",
      " 54%|█████▎    | 241M/448M [00:13<00:04, 46.0MB/s]\n",
      " 55%|█████▍    | 246M/448M [00:13<00:04, 42.0MB/s]\n",
      " 56%|█████▌    | 252M/448M [00:13<00:04, 45.5MB/s]\n",
      " 58%|█████▊    | 258M/448M [00:13<00:03, 49.2MB/s]\n",
      " 59%|█████▉    | 264M/448M [00:13<00:03, 51.9MB/s]\n",
      " 60%|██████    | 270M/448M [00:13<00:03, 49.8MB/s]\n",
      " 62%|██████▏   | 276M/448M [00:13<00:03, 51.8MB/s]\n",
      " 63%|██████▎   | 282M/448M [00:13<00:03, 50.5MB/s]\n",
      " 64%|██████▍   | 287M/448M [00:14<00:03, 49.8MB/s]\n",
      " 65%|██████▌   | 293M/448M [00:14<00:03, 51.4MB/s]\n",
      " 67%|██████▋   | 298M/448M [00:14<00:02, 52.7MB/s]\n",
      " 68%|██████▊   | 304M/448M [00:14<00:02, 54.0MB/s]\n",
      " 69%|██████▉   | 310M/448M [00:14<00:03, 36.3MB/s]\n",
      " 70%|███████   | 316M/448M [00:14<00:03, 40.4MB/s]\n",
      " 72%|███████▏  | 322M/448M [00:14<00:02, 44.9MB/s]\n",
      " 73%|███████▎  | 328M/448M [00:15<00:02, 48.4MB/s]\n",
      " 74%|███████▍  | 334M/448M [00:15<00:02, 50.8MB/s]\n",
      " 76%|███████▌  | 340M/448M [00:15<00:02, 53.1MB/s]\n",
      " 77%|███████▋  | 346M/448M [00:15<00:03, 31.7MB/s]\n",
      " 79%|███████▊  | 352M/448M [00:15<00:02, 37.1MB/s]\n",
      " 80%|███████▉  | 358M/448M [00:15<00:02, 37.9MB/s]\n",
      " 81%|████████▏ | 365M/448M [00:15<00:01, 46.7MB/s]\n",
      " 83%|████████▎ | 372M/448M [00:16<00:01, 49.5MB/s]\n",
      " 84%|████████▍ | 377M/448M [00:16<00:01, 51.4MB/s]\n",
      " 85%|████████▌ | 383M/448M [00:16<00:01, 49.5MB/s]\n",
      " 87%|████████▋ | 390M/448M [00:16<00:01, 51.5MB/s]\n",
      " 88%|████████▊ | 395M/448M [00:16<00:01, 52.0MB/s]\n",
      " 89%|████████▉ | 401M/448M [00:16<00:00, 53.2MB/s]\n",
      " 91%|█████████ | 407M/448M [00:16<00:00, 54.0MB/s]\n",
      " 92%|█████████▏| 413M/448M [00:16<00:00, 54.3MB/s]\n",
      " 93%|█████████▎| 418M/448M [00:16<00:00, 55.2MB/s]\n",
      " 95%|█████████▍| 425M/448M [00:16<00:00, 56.4MB/s]\n",
      " 96%|█████████▌| 431M/448M [00:17<00:00, 57.2MB/s]\n",
      " 97%|█████████▋| 437M/448M [00:17<00:00, 57.0MB/s]\n",
      " 99%|█████████▊| 442M/448M [00:17<00:00, 56.7MB/s]\n",
      "100%|██████████| 448M/448M [00:17<00:00, 57.2MB/s]\n",
      "100%|██████████| 448M/448M [00:17<00:00, 25.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1M0yYP2umxlwaAbk_iq5G_Z5y3qLu9Wet\n",
      "To: c:\\Users\\hoya9\\OneDrive\\Desktop\\torch-gpu\\Workspace\\Transformer-pytorch\\data\\Transformer_small_history.pt\n",
      "\n",
      "  0%|          | 0.00/1.39k [00:00<?, ?B/s]\n",
      "100%|██████████| 1.39k/1.39k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoya9\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hoya9\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-ko-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64 # 실제 논문에선 2.5만 token이 한 batch에 담기게 했다고 함\n",
    "LAMBDA = 0 # l2-Regularization를 위한 hyperparams (저장된 모델)\n",
    "EPOCH = 15 # 저장된 모델\n",
    "max_len = 100 # 길이 제한 (GPU 부담도 많이 감소)\n",
    "\"\"\"\n",
    "decoder에서 모델이 <eos>가 출력되는 것까지는 Loss를 구하고,\n",
    "그뒤 <pad>에 대한 Loss는 무시하기 위한 역활\n",
    "즉, label이 <pad> 일 때는 무시\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "scheduler_name = 'Noam' # Transformer 논문 제 2저자가 제안한 learning rate scheduler\n",
    "scheduler_name = 'Cos'\n",
    "#### Noam ####\n",
    "# warmup_steps = 4000 # 실제 논문에서 제시한 값 (총 10만 step의 4%)\n",
    "warmup_steps = 1000 # 데이터 수 * EPOCH / BS = 총 step 수 인것 고려 (저장된 모델)\n",
    "LR_scale = .5 # Noam scheduler에 peak LR 값 조절을 위해 곱해질 값 (저장된 모델)\n",
    "#### Cos ####\n",
    "LR_init = 5e-4\n",
    "T0 = 1500 # 첫 주기\n",
    "T_mult = 2 # 배 만큼 주기가 길어짐 (1보다 큰 정수여야 함)\n",
    "#############\n",
    "\n",
    "\"\"\"\n",
    "나만의 모델을 만들고 싶으면,\n",
    "new_model_train = True\n",
    "prev_model_use = False\n",
    "\"\"\"\n",
    "new_model_train = False\n",
    "prev_model_use = True\n",
    "\n",
    "!mkdir data\n",
    "\n",
    "if prev_model_use:\n",
    "    !gdown https://drive.google.com/uc?id=1bjbeWgqlVKJ9gzDcL1hfD9cIItQy9_N2 -O data/Transformer_small.pt\n",
    "    !gdown https://drive.google.com/uc?id=1M0yYP2umxlwaAbk_iq5G_Z5y3qLu9Wet -O data/Transformer_small_history.pt\n",
    "\n",
    "    save_model_path = 'data/Transformer_small.pt'\n",
    "    save_history_path = 'data/Transformer_small_history.pt'\n",
    "else:\n",
    "    save_model=path = 'data/Transformer_small2.pt'\n",
    "    save_history_path = 'data/Transformer_small2_history.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1803d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "논문에 나오는 BASE MODEL\n",
    "train loss를 많이 줄이려면 많은 epoch이 요구됨.\n",
    "test 성능도 높이려면 더 많은 데이터가 필요함\n",
    "\"\"\"\n",
    "n_layers = 6\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n_heads = 8\n",
    "drop_p = .1\n",
    "\n",
    "# 좀 사이즈 줄인 모델 (훈련된 input_embedding, fc_out 사용하면 사용 불가)\n",
    "n_layers = 3\n",
    "d_model = 26\n",
    "d_ff = 512\n",
    "n_heads = 8\n",
    "drop_p = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bcc2e5",
   "metadata": {},
   "source": [
    "### 토크나이저 & 학습된 모델 써보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0d2ecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁H', 'i', ',', '▁I', \"'\", 'm', '▁I', 'an', '.', '▁', '...', '▁a', '▁a', '?']\n",
      "['▁a', '/', 'b', '▁1', '+2', '+3', '▁2:', '1', '▁a', '>', 'b']\n",
      "['▁p', 're', 'tra', 'in', 'ed', '▁re', 'st', 'art']\n",
      "['▁ch', 'at', 'G', 'P', 'T']\n",
      "['▁The', '▁', 'ex', 'am', 'ple', '▁is', '▁', 'very', '▁good', '▁in', '▁', 'our', '▁', 'le', 'c', 'ture']\n",
      "['▁한', '글', '은', '▁어떻게', '▁할까', '?']\n",
      "['▁확실히', '▁띄', '어', '쓰기', '▁기준으로', '▁토', '크', '나이', '징', '을', '▁하는', '▁것', '▁같', '진', '▁않다', '.']\n",
      "['▁여러분', '▁차례', '!']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 써보기 (_로 띄어쓰기를 나타낸다! 즉, _가 없으면 이어진 한 단어 : subword tokenizing)\n",
    "# tokenizer에 대한 참고 자료: https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/\n",
    "print(tokenizer.tokenize(\"Hi, I'm Ian. ... a  a?\"))\n",
    "print(tokenizer.tokenize('a/b 1+2+3 2:1 a>b'))\n",
    "print(tokenizer.tokenize('pretrained restart'))\n",
    "print(tokenizer.tokenize('chatGPT'))\n",
    "print(tokenizer.tokenize('The example is very good in our lecture'))\n",
    "print(tokenizer.tokenize('한글은 어떻게 할까?'))\n",
    "print(tokenizer.tokenize('확실히 띄어쓰기 기준으로 토크나이징을 하는 것 같진 않다.'))\n",
    "print(tokenizer.tokenize('여러분 차례!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed4cd73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65001\n",
      "[34359]\n",
      "[65000]\n",
      "[0]\n",
      "[125]\n",
      "[59]\n",
      "['▁문장', '을', '▁넣으면', '▁to', 'k', 'en', 'ize', '해서', '▁숫자', '로', '▁바꾼', '다']\n",
      "[13774, 51, 40068, 5, 1479, 1252, 5016, 969, 6635, 131, 30737, 161]\n",
      "사람\n",
      "사람\n",
      "사람\n",
      "</s> <unk> ., the to of? and s a' in 들<pad> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.get_vocab()) # tokenizer에 들어있는 모든 토큰과 토큰의 해당하는 번호를 dict으로 가짐\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print((vocab_size))\n",
    "\n",
    "# add_special_token은 <eos> 자동 붙여주는 것을 방지\n",
    "print(tokenizer.encode('지능', add_special_tokens=False))   # string to index\n",
    "print(tokenizer.encode('<pad>', add_special_tokens=False))  # <pad>는 65000\n",
    "print(tokenizer.encode('</s>', add_special_tokens=False))   # <sos> or <eos>는 0\n",
    "print(tokenizer.encode('He', add_special_tokens=False))\n",
    "print(tokenizer.encode('he', add_special_tokens=False))     # 대소문자 다른 단어로 인식\n",
    "print(tokenizer.tokenize('문장을 넣으면 tokenize해서 숫자로 바꾼다'))\n",
    "print(tokenizer.encode('문장을 넣으면 tokenize해서 숫자로 바꾼다', add_special_tokens=False))\n",
    "print(tokenizer.decode([204]))\n",
    "print(tokenizer.decode([204]))\n",
    "print(tokenizer.decode([204]))\n",
    "print(tokenizer.decode(list(range(15)) + [65000, 65001, 65002, 65003]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1244ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
