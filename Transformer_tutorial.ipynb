{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8296e231-f46d-4646-8821-6a188a7bb1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family=\"NanumBarunGothic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab065354-2ca1-4fe1-9eb3-c039534616b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoya9\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from transformers import MarianMTModel, MarianTokenizer # translation machine\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math, random\n",
    "from einops import rearrange\n",
    "\n",
    "DEVICE = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c8bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # Multi-GPU case\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da12c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer & model\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n",
    "model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ko-en') # MT: Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158f7887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_idx =  0\n",
      "pad_idx =  65000\n"
     ]
    }
   ],
   "source": [
    "eos_idx = tokenizer.eos_token_id\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "print('eos_idx = ', eos_idx)\n",
    "print('pad_idx = ', pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b90c7",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc9f2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file data already exists.\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1bjbeWgqlVKJ9gzDcL1hfD9cIItQy9_N2\n",
      "From (redirected): https://drive.google.com/uc?id=1bjbeWgqlVKJ9gzDcL1hfD9cIItQy9_N2&confirm=t&uuid=300c6135-66a6-47c4-afa7-db96169eb7b7\n",
      "To: c:\\Users\\hoya9\\OneDrive\\Desktop\\torch-gpu\\Workspace\\Transformer-pytorch\\data\\Transformer_small.pt\n",
      "\n",
      "  0%|          | 0.00/448M [00:00<?, ?B/s]\n",
      "  0%|          | 524k/448M [00:00<03:26, 2.16MB/s]\n",
      "  0%|          | 2.10M/448M [00:00<01:04, 6.95MB/s]\n",
      "  1%|▏         | 5.77M/448M [00:00<00:27, 16.1MB/s]\n",
      "  2%|▏         | 7.86M/448M [00:00<00:26, 16.9MB/s]\n",
      "  2%|▏         | 10.5M/448M [00:00<00:22, 19.4MB/s]\n",
      "  3%|▎         | 13.1M/448M [00:00<00:21, 19.8MB/s]\n",
      "  4%|▎         | 15.7M/448M [00:00<00:22, 19.5MB/s]\n",
      "  4%|▍         | 17.8M/448M [00:01<00:27, 15.8MB/s]\n",
      "  4%|▍         | 19.9M/448M [00:01<00:28, 15.0MB/s]\n",
      "  5%|▍         | 22.0M/448M [00:01<00:28, 14.8MB/s]\n",
      "  5%|▌         | 23.6M/448M [00:01<00:28, 14.9MB/s]\n",
      "  6%|▌         | 25.7M/448M [00:01<00:27, 15.3MB/s]\n",
      "  6%|▌         | 27.8M/448M [00:01<00:31, 13.3MB/s]\n",
      "  7%|▋         | 30.4M/448M [00:01<00:26, 15.9MB/s]\n",
      "  7%|▋         | 32.5M/448M [00:02<00:24, 16.7MB/s]\n",
      "  8%|▊         | 34.6M/448M [00:02<00:32, 12.9MB/s]\n",
      "  9%|▊         | 38.3M/448M [00:02<00:39, 10.3MB/s]\n",
      " 10%|▉         | 43.5M/448M [00:02<00:24, 16.4MB/s]\n",
      " 10%|█         | 46.7M/448M [00:03<00:22, 18.2MB/s]\n",
      " 11%|█         | 49.3M/448M [00:03<00:22, 17.8MB/s]\n",
      " 12%|█▏        | 53.0M/448M [00:03<00:26, 14.7MB/s]\n",
      " 13%|█▎        | 58.2M/448M [00:03<00:19, 20.5MB/s]\n",
      " 14%|█▍        | 61.9M/448M [00:03<00:16, 23.1MB/s]\n",
      " 15%|█▍        | 67.1M/448M [00:03<00:13, 28.9MB/s]\n",
      " 16%|█▌        | 71.3M/448M [00:03<00:12, 29.8MB/s]\n",
      " 17%|█▋        | 75.0M/448M [00:04<00:14, 26.6MB/s]\n",
      " 17%|█▋        | 78.1M/448M [00:04<00:15, 24.5MB/s]\n",
      " 18%|█▊        | 81.3M/448M [00:04<00:19, 18.6MB/s]\n",
      " 19%|█▉        | 84.4M/448M [00:04<00:22, 16.2MB/s]\n",
      " 20%|█▉        | 88.6M/448M [00:04<00:17, 20.2MB/s]\n",
      " 20%|██        | 91.8M/448M [00:05<00:16, 21.5MB/s]\n",
      " 21%|██        | 94.9M/448M [00:05<00:16, 22.0MB/s]\n",
      " 22%|██▏       | 97.5M/448M [00:05<00:19, 18.2MB/s]\n",
      " 22%|██▏       | 101M/448M [00:05<00:16, 20.5MB/s] \n",
      " 23%|██▎       | 104M/448M [00:05<00:15, 22.6MB/s]\n",
      " 24%|██▎       | 106M/448M [00:05<00:15, 21.5MB/s]\n",
      " 24%|██▍       | 109M/448M [00:05<00:15, 22.4MB/s]\n",
      " 25%|██▌       | 112M/448M [00:06<00:13, 24.4MB/s]\n",
      " 26%|██▌       | 116M/448M [00:06<00:13, 25.1MB/s]\n",
      " 26%|██▋       | 118M/448M [00:06<00:15, 21.7MB/s]\n",
      " 27%|██▋       | 121M/448M [00:06<00:14, 22.3MB/s]\n",
      " 28%|██▊       | 124M/448M [00:06<00:15, 21.2MB/s]\n",
      " 28%|██▊       | 126M/448M [00:06<00:16, 19.3MB/s]\n",
      " 29%|██▉       | 131M/448M [00:06<00:13, 23.2MB/s]\n",
      " 30%|██▉       | 133M/448M [00:07<00:16, 19.2MB/s]\n",
      " 30%|███       | 136M/448M [00:07<00:15, 19.9MB/s]\n",
      " 31%|███       | 138M/448M [00:07<00:16, 18.7MB/s]\n",
      " 32%|███▏      | 143M/448M [00:07<00:12, 23.5MB/s]\n",
      " 32%|███▏      | 145M/448M [00:07<00:12, 23.5MB/s]\n",
      " 33%|███▎      | 149M/448M [00:07<00:11, 27.1MB/s]\n",
      " 34%|███▍      | 153M/448M [00:07<00:10, 27.6MB/s]\n",
      " 35%|███▍      | 156M/448M [00:07<00:10, 27.2MB/s]\n",
      " 35%|███▌      | 159M/448M [00:08<00:11, 25.7MB/s]\n",
      " 37%|███▋      | 165M/448M [00:08<00:08, 33.2MB/s]\n",
      " 38%|███▊      | 169M/448M [00:08<00:08, 34.5MB/s]\n",
      " 38%|███▊      | 172M/448M [00:08<00:09, 29.3MB/s]\n",
      " 39%|███▉      | 176M/448M [00:08<00:12, 22.6MB/s]\n",
      " 40%|████      | 180M/448M [00:08<00:10, 26.1MB/s]\n",
      " 41%|████      | 184M/448M [00:08<00:10, 26.4MB/s]\n",
      " 42%|████▏     | 187M/448M [00:09<00:11, 21.9MB/s]\n",
      " 43%|████▎     | 191M/448M [00:09<00:09, 25.9MB/s]\n",
      " 43%|████▎     | 195M/448M [00:09<00:09, 27.8MB/s]\n",
      " 44%|████▍     | 198M/448M [00:09<00:09, 25.8MB/s]\n",
      " 45%|████▍     | 201M/448M [00:09<00:10, 24.2MB/s]\n",
      " 46%|████▌     | 206M/448M [00:09<00:08, 28.1MB/s]\n",
      " 47%|████▋     | 210M/448M [00:09<00:07, 32.5MB/s]\n",
      " 48%|████▊     | 215M/448M [00:09<00:06, 37.0MB/s]\n",
      " 49%|████▉     | 220M/448M [00:10<00:05, 38.9MB/s]\n",
      " 50%|█████     | 225M/448M [00:10<00:05, 41.0MB/s]\n",
      " 51%|█████     | 230M/448M [00:10<00:05, 42.3MB/s]\n",
      " 52%|█████▏    | 234M/448M [00:10<00:06, 31.3MB/s]\n",
      " 53%|█████▎    | 238M/448M [00:10<00:07, 28.8MB/s]\n",
      " 54%|█████▍    | 242M/448M [00:10<00:06, 29.8MB/s]\n",
      " 55%|█████▍    | 245M/448M [00:10<00:07, 28.7MB/s]\n",
      " 55%|█████▌    | 249M/448M [00:11<00:07, 27.2MB/s]\n",
      " 56%|█████▌    | 252M/448M [00:11<00:07, 26.0MB/s]\n",
      " 57%|█████▋    | 255M/448M [00:11<00:07, 27.2MB/s]\n",
      " 58%|█████▊    | 258M/448M [00:11<00:06, 27.4MB/s]\n",
      " 58%|█████▊    | 262M/448M [00:11<00:05, 31.2MB/s]\n",
      " 59%|█████▉    | 266M/448M [00:11<00:05, 33.7MB/s]\n",
      " 60%|██████    | 270M/448M [00:11<00:05, 32.3MB/s]\n",
      " 61%|██████    | 274M/448M [00:12<00:08, 19.9MB/s]\n",
      " 62%|██████▏   | 278M/448M [00:12<00:07, 23.3MB/s]\n",
      " 63%|██████▎   | 283M/448M [00:12<00:05, 28.9MB/s]\n",
      " 64%|██████▍   | 287M/448M [00:12<00:06, 26.3MB/s]\n",
      " 65%|██████▍   | 291M/448M [00:12<00:05, 29.2MB/s]\n",
      " 66%|██████▌   | 295M/448M [00:12<00:05, 28.0MB/s]\n",
      " 67%|██████▋   | 300M/448M [00:12<00:04, 33.3MB/s]\n",
      " 68%|██████▊   | 304M/448M [00:13<00:06, 21.1MB/s]\n",
      " 68%|██████▊   | 307M/448M [00:13<00:08, 16.4MB/s]\n",
      " 69%|██████▉   | 309M/448M [00:13<00:08, 17.0MB/s]\n",
      " 70%|███████   | 315M/448M [00:13<00:05, 22.7MB/s]\n",
      " 71%|███████   | 318M/448M [00:13<00:06, 19.3MB/s]\n",
      " 71%|███████▏  | 320M/448M [00:14<00:07, 17.9MB/s]\n",
      " 73%|███████▎  | 326M/448M [00:14<00:05, 22.6MB/s]\n",
      " 74%|███████▍  | 331M/448M [00:14<00:04, 28.2MB/s]\n",
      " 75%|███████▍  | 334M/448M [00:14<00:04, 26.3MB/s]\n",
      " 76%|███████▌  | 340M/448M [00:14<00:03, 31.9MB/s]\n",
      " 77%|███████▋  | 344M/448M [00:14<00:03, 29.0MB/s]\n",
      " 78%|███████▊  | 348M/448M [00:15<00:03, 26.2MB/s]\n",
      " 78%|███████▊  | 352M/448M [00:15<00:03, 29.5MB/s]\n",
      " 79%|███████▉  | 355M/448M [00:15<00:03, 27.9MB/s]\n",
      " 80%|████████  | 360M/448M [00:15<00:02, 30.9MB/s]\n",
      " 81%|████████  | 363M/448M [00:15<00:03, 26.7MB/s]\n",
      " 82%|████████▏ | 368M/448M [00:15<00:02, 30.9MB/s]\n",
      " 83%|████████▎ | 372M/448M [00:15<00:02, 30.8MB/s]\n",
      " 84%|████████▎ | 375M/448M [00:15<00:02, 31.5MB/s]\n",
      " 85%|████████▍ | 379M/448M [00:16<00:02, 29.3MB/s]\n",
      " 86%|████████▌ | 384M/448M [00:16<00:01, 33.7MB/s]\n",
      " 87%|████████▋ | 388M/448M [00:16<00:01, 34.4MB/s]\n",
      " 87%|████████▋ | 392M/448M [00:16<00:01, 35.6MB/s]\n",
      " 88%|████████▊ | 396M/448M [00:16<00:01, 27.1MB/s]\n",
      " 89%|████████▉ | 400M/448M [00:16<00:01, 29.9MB/s]\n",
      " 90%|█████████ | 404M/448M [00:16<00:01, 31.5MB/s]\n",
      " 91%|█████████ | 408M/448M [00:17<00:01, 28.2MB/s]\n",
      " 92%|█████████▏| 412M/448M [00:17<00:01, 29.7MB/s]\n",
      " 93%|█████████▎| 416M/448M [00:17<00:01, 26.4MB/s]\n",
      " 94%|█████████▎| 419M/448M [00:17<00:01, 26.2MB/s]\n",
      " 95%|█████████▍| 424M/448M [00:17<00:00, 29.5MB/s]\n",
      " 95%|█████████▌| 427M/448M [00:17<00:00, 28.8MB/s]\n",
      " 96%|█████████▌| 430M/448M [00:17<00:00, 28.6MB/s]\n",
      " 97%|█████████▋| 434M/448M [00:17<00:00, 26.5MB/s]\n",
      " 98%|█████████▊| 438M/448M [00:18<00:00, 30.3MB/s]\n",
      " 98%|█████████▊| 441M/448M [00:18<00:00, 31.9MB/s]\n",
      " 99%|█████████▉| 445M/448M [00:18<00:00, 32.8MB/s]\n",
      "100%|██████████| 448M/448M [00:18<00:00, 24.5MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1M0yYP2umxlwaAbk_iq5G_Z5y3qLu9Wet\n",
      "To: c:\\Users\\hoya9\\OneDrive\\Desktop\\torch-gpu\\Workspace\\Transformer-pytorch\\data\\Transformer_small_history.pt\n",
      "\n",
      "  0%|          | 0.00/1.39k [00:00<?, ?B/s]\n",
      "100%|██████████| 1.39k/1.39k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64 # 실제 논문에선 2.5만 token이 한 batch에 담기게 했다고 함\n",
    "LAMBDA = 0 # l2-Regularization를 위한 hyperparams (저장된 모델)\n",
    "EPOCH = 15 # 저장된 모델\n",
    "max_len = 100 # 길이 제한 (GPU 부담도 많이 감소)\n",
    "\"\"\"\n",
    "decoder에서 모델이 <eos>가 출력되는 것까지는 Loss를 구하고,\n",
    "그뒤 <pad>에 대한 Loss는 무시하기 위한 역활\n",
    "즉, label이 <pad> 일 때는 무시\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "scheduler_name = 'Noam' # Transformer 논문 제 2저자가 제안한 learning rate scheduler\n",
    "scheduler_name = 'Cos'\n",
    "#### Noam ####\n",
    "# warmup_steps = 4000 # 실제 논문에서 제시한 값 (총 10만 step의 4%)\n",
    "warmup_steps = 1000 # 데이터 수 * EPOCH / BS = 총 step 수 인것 고려 (저장된 모델)\n",
    "LR_scale = .5 # Noam scheduler에 peak LR 값 조절을 위해 곱해질 값 (저장된 모델)\n",
    "#### Cos ####\n",
    "LR_init = 5e-4\n",
    "T0 = 1500 # 첫 주기\n",
    "T_mult = 2 # 배 만큼 주기가 길어짐 (1보다 큰 정수여야 함)\n",
    "#############\n",
    "\n",
    "\"\"\"\n",
    "나만의 모델을 만들고 싶으면,\n",
    "new_model_train = True\n",
    "prev_model_use = False\n",
    "\"\"\"\n",
    "new_model_train = False\n",
    "prev_model_use = True\n",
    "\n",
    "!mkdir data\n",
    "\n",
    "if prev_model_use:\n",
    "    !gdown https://drive.google.com/uc?id=1bjbeWgqlVKJ9gzDcL1hfD9cIItQy9_N2 -O data/Transformer_small.pt\n",
    "    !gdown https://drive.google.com/uc?id=1M0yYP2umxlwaAbk_iq5G_Z5y3qLu9Wet -O data/Transformer_small_history.pt\n",
    "\n",
    "    save_model_path = 'data/Transformer_small.pt'\n",
    "    save_history_path = 'data/Transformer_small_history.pt'\n",
    "else:\n",
    "    save_model=path = 'data/Transformer_small2.pt'\n",
    "    save_history_path = 'data/Transformer_small2_history.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1803d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "논문에 나오는 BASE MODEL\n",
    "train loss를 많이 줄이려면 많은 epoch이 요구됨.\n",
    "test 성능도 높이려면 더 많은 데이터가 필요함\n",
    "\"\"\"\n",
    "n_layers = 6\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n_heads = 8\n",
    "drop_p = .1\n",
    "\n",
    "# 좀 사이즈 줄인 모델 (훈련된 input_embedding, fc_out 사용하면 사용 불가)\n",
    "n_layers = 3\n",
    "d_model = 26\n",
    "d_ff = 512\n",
    "n_heads = 8\n",
    "drop_p = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bcc2e5",
   "metadata": {},
   "source": [
    "### 토크나이저 & 학습된 모델 써보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0d2ecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁H', 'i', ',', '▁I', \"'\", 'm', '▁I', 'an', '.', '▁', '...', '▁a', '▁a', '?']\n",
      "['▁a', '/', 'b', '▁1', '+2', '+3', '▁2:', '1', '▁a', '>', 'b']\n",
      "['▁p', 're', 'tra', 'in', 'ed', '▁re', 'st', 'art']\n",
      "['▁ch', 'at', 'G', 'P', 'T']\n",
      "['▁The', '▁', 'ex', 'am', 'ple', '▁is', '▁', 'very', '▁good', '▁in', '▁', 'our', '▁', 'le', 'c', 'ture']\n",
      "['▁한', '글', '은', '▁어떻게', '▁할까', '?']\n",
      "['▁확실히', '▁띄', '어', '쓰기', '▁기준으로', '▁토', '크', '나이', '징', '을', '▁하는', '▁것', '▁같', '진', '▁않다', '.']\n",
      "['▁여러분', '▁차례', '!']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 써보기 (_로 띄어쓰기를 나타낸다! 즉, _가 없으면 이어진 한 단어 : subword tokenizing)\n",
    "# tokenizer에 대한 참고 자료: https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/\n",
    "print(tokenizer.tokenize(\"Hi, I'm Ian. ... a  a?\"))\n",
    "print(tokenizer.tokenize('a/b 1+2+3 2:1 a>b'))\n",
    "print(tokenizer.tokenize('pretrained restart'))\n",
    "print(tokenizer.tokenize('chatGPT'))\n",
    "print(tokenizer.tokenize('The example is very good in our lecture'))\n",
    "print(tokenizer.tokenize('한글은 어떻게 할까?'))\n",
    "print(tokenizer.tokenize('확실히 띄어쓰기 기준으로 토크나이징을 하는 것 같진 않다.'))\n",
    "print(tokenizer.tokenize('여러분 차례!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed4cd73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65001\n",
      "[34359]\n",
      "[65000]\n",
      "[0]\n",
      "[125]\n",
      "[59]\n",
      "['▁문장', '을', '▁넣으면', '▁to', 'k', 'en', 'ize', '해서', '▁숫자', '로', '▁바꾼', '다']\n",
      "[13774, 51, 40068, 5, 1479, 1252, 5016, 969, 6635, 131, 30737, 161]\n",
      "사람\n",
      "으로\n",
      "make\n",
      "</s> <unk> ., the to of? and s a' in 들<pad> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.get_vocab()) # tokenizer에 들어있는 모든 토큰과 토큰의 해당하는 번호를 dict으로 가짐\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print((vocab_size))\n",
    "\n",
    "# add_special_token은 <eos> 자동 붙여주는 것을 방지\n",
    "print(tokenizer.encode('지능', add_special_tokens=False))   # string to index\n",
    "print(tokenizer.encode('<pad>', add_special_tokens=False))  # <pad>는 65000\n",
    "print(tokenizer.encode('</s>', add_special_tokens=False))   # <sos> or <eos>는 0\n",
    "print(tokenizer.encode('He', add_special_tokens=False))\n",
    "print(tokenizer.encode('he', add_special_tokens=False))     # 대소문자 다른 단어로 인식\n",
    "print(tokenizer.tokenize('문장을 넣으면 tokenize해서 숫자로 바꾼다'))\n",
    "print(tokenizer.encode('문장을 넣으면 tokenize해서 숫자로 바꾼다', add_special_tokens=False))\n",
    "print(tokenizer.decode([204]))\n",
    "print(tokenizer.decode([206]))\n",
    "print(tokenizer.decode([210]))\n",
    "print(tokenizer.decode(list(range(15)) + [65000, 65001, 65002, 65003]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1244ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[481, 948, 41908, 161, 66, 46249, 13204, 19381, 28, 0]\n",
      "tensor([[  481,   948, 41908,   161,    66, 46249, 13204, 19381,    28,     0]])\n"
     ]
    }
   ],
   "source": [
    "# 사전 학습된 모델로 번역\n",
    "input_text = \"지금 너무 배고프다... 치킨먹고 싶다!\"\n",
    "\n",
    "input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(input_tokens, max_new_tokens=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1513ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-GPU",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
